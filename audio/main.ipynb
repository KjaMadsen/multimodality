{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dataset import AudioDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model2 import EmotionRecognizer, ConvNet\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "from model import LSTMnet_RnnAtten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = '/Users/zuzia/Downloads/MELD.Raw/train/train_sent_emo.csv'\n",
    "val_annotations = '/Users/zuzia/Downloads/MELD.Raw/dev_sent_emo.csv'\n",
    "\n",
    "train_audio = '/Users/zuzia/Downloads/MELD.Raw/train/train_splits/wav'\n",
    "val_audio = '/Users/zuzia/Downloads/MELD.Raw/dev_splits_complete/wav'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = AudioDataset(train_annotations, train_audio, transform = None)\n",
    "val = AudioDataset(val_annotations, val_audio, transform = None)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=32, shuffle = True, drop_last=True)\n",
    "val_dataloader = DataLoader(val, batch_size=32, shuffle=True, drop_last=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9988"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 100])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 88.2079,  60.0846,   6.6171,  14.8374,  -3.0069,  -2.0702, -15.3161,\n",
       "          -13.9392,  -5.0488,  -4.0143,  -7.3686,  -3.9030,  -2.7670,   4.9882,\n",
       "           -3.1765,   3.0647,  -0.3962,  -4.9282,  -2.7082,  -4.1611,  -2.7944,\n",
       "           -0.6329,  -3.3335,  -4.6907,  -5.8244,  -3.3087,  -7.7267,  -5.5704,\n",
       "           -4.2631,  -1.4815,   2.0238,   7.8157,   9.3493,   7.5352,   3.4834,\n",
       "           -2.0878,  -0.4129,   0.1249,   1.1677,   1.0979,  -2.4662,  -0.7578,\n",
       "           -0.9688,  -0.4189,  -1.5832,  -2.3393,  -1.5784,  -0.5006,  -1.3474,\n",
       "           -1.2672,  -0.9041,  -0.6816,  -3.1787,  -1.4073,  -1.0606,  -1.9261,\n",
       "           -2.8761,  -2.0089,  -1.2797,  -1.2950,   0.2506,   1.5064,   0.2917,\n",
       "           -0.6703,   0.3881,   1.8584,   1.5484,   0.6946,   0.7729,  -1.2744,\n",
       "           -1.5535,   0.5202,   0.8158,  -0.6739,  -0.4841,  -0.6081,  -0.7773,\n",
       "           -0.2676,  -0.4270,  -1.0843,  -1.2439,   0.2346,   0.1006,  -0.2424,\n",
       "           -0.0979,  -0.3875,  -0.3077,   0.1087,   0.3826,   0.1308,   0.2828,\n",
       "           -0.2171,  -0.5871,   0.1313,   0.1199,  -0.2739,   0.2120,  -0.1510,\n",
       "           -0.3079,   0.3015]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: Size should be [batch_size, num_features, feature_vector_len], [batch_size]\n",
    "# Where batch size is 32, num_features (num mfccs) is 40 \n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "features = train_features[0]\n",
    "label = train_labels[0]\n",
    "features,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionRecognizer(in_feat=train.num_features, num_classes=train.num_classes, p_dropout=0.0, lr=1e-4)\n",
    "print(model)\n",
    "print('num params:', model.count_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    x, y = batch\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(save_dir='logs/',name='cnn_logs')\n",
    "trainer = Trainer(max_epochs=150, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet()\n",
    "\n",
    "#### 0.03 -> 1e-3\n",
    "optimizer = optim.Adam(net.parameters(), lr=2e-5)\n",
    "\n",
    "#### BCELoss -> CrossEntropyLoss\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "steps = 0\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    #### put net in train mode\n",
    "    net.train()\n",
    "    for idx, (features, labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = net(features)\n",
    "        loss = loss_function(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        #### put net in eval mode\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_dataloader:\n",
    "                log_ps = net(features)\n",
    "                test_loss += loss_function(log_ps, labels)\n",
    "                #### removed torch.exp() since exponential is monotone, taking it doesn't change the order of outputs. Similarly with torch.softmax()\n",
    "                top_p, top_class = log_ps.topk(1, dim=1)\n",
    "                #### convert to float/long using proper methods. what you have won't work for cuda tensors.\n",
    "                equals = top_class.long() == labels.long().view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.float())\n",
    "        train_losses.append(running_loss/len(train_dataloader))\n",
    "        test_losses.append(test_loss/len(val_dataloader))\n",
    "        print(\"[Epoch: {}/{}] \".format(e+1, epochs),\n",
    "              \"[Training Loss: {:.3f}] \".format(running_loss/len(val_dataloader)),\n",
    "              \"[Test Loss: {:.3f}] \".format(test_loss/len(val_dataloader)),\n",
    "              \"[Test Accuracy: {:.3f}]\".format(accuracy/len(val_dataloader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits = model(X)\n",
    "        predictions = torch.argmax(output_logits,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits = model(X)\n",
    "            predictions = torch.argmax(output_logits,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable params:  1406471\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = LSTMnet_RnnAtten(input_dim = 100, hidden_dim=128, output_dim=7, num_layers=10).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(),lr=0.0001, weight_decay=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0: batch 311\n",
      "Epoch 0 --> loss:1.7783, acc:36.39%, val_loss:1.8624, val_acc:46.88%\n",
      " Epoch 1: batch 122"
     ]
    }
   ],
   "source": [
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "\n",
    "losses=[]\n",
    "val_losses = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    for idx, (features, labels) in enumerate(train_dataloader):\n",
    "        X, Y = features, labels\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*len(features)/len(train)\n",
    "        epoch_loss += loss*len(features)/len(train)\n",
    "        print(f\"\\r Epoch {epoch}: batch {idx}\",end='')\n",
    "    for batch in val_dataloader:\n",
    "        X_val, Y_val = batch\n",
    "        break\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40a9768d250eb22e51b6c5a3b41fdf4e51420751aaeb4dd105d54b23da568f3d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
