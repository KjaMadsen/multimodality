{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dataset import AudioDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "from model import LSTMnet_RnnAtten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = '/Users/zuzia/Downloads/MELD.Raw/train/train_sent_emo.csv'\n",
    "val_annotations = '/Users/zuzia/Downloads/MELD.Raw/dev_sent_emo.csv'\n",
    "\n",
    "train_audio = '/Users/zuzia/Downloads/MELD.Raw/train/train_splits/wav'\n",
    "val_audio = '/Users/zuzia/Downloads/MELD.Raw/dev_splits_complete/wav'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = AudioDataset(train_annotations, train_audio)\n",
    "val = AudioDataset(val_annotations, val_audio)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=32, shuffle = True, drop_last=True)\n",
    "val_dataloader = DataLoader(val, batch_size=32, shuffle=True, drop_last=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9988"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([32, 1, 100])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.1128e+01,  2.9132e+01, -6.6171e+00,  2.6409e+00, -7.9006e+00,\n",
       "          -9.9457e+00, -7.1557e+00, -4.0811e+00, -3.2150e+00, -3.0161e+00,\n",
       "          -1.6573e+00, -4.6688e+00,  1.2981e+00,  2.6853e+00, -1.3311e+00,\n",
       "           2.2540e+00, -2.2698e+00, -2.2175e+00, -2.1314e+00, -3.0604e+00,\n",
       "          -1.8158e+00, -1.4692e+00, -4.7822e+00, -6.1581e+00, -6.2343e+00,\n",
       "          -1.2349e+00,  3.7591e+00,  4.5148e+00,  4.1583e+00,  4.6243e+00,\n",
       "           4.1283e+00,  2.2471e+00, -5.3318e-01, -8.5891e-02, -3.8426e-01,\n",
       "           3.2498e-01,  1.2366e+00, -7.2077e-01, -5.6705e-01,  2.1665e-01,\n",
       "          -2.5582e+00, -3.4500e+00, -3.0593e+00, -2.1821e+00, -2.5977e+00,\n",
       "          -3.2442e+00, -3.0872e+00, -2.3810e+00, -2.2971e+00, -2.1400e+00,\n",
       "          -2.2100e+00, -2.1986e+00, -6.4927e-01,  3.7317e-01, -9.7138e-02,\n",
       "           5.6490e-01,  1.6908e+00,  1.0802e+00,  3.6913e-01,  9.4291e-01,\n",
       "           7.5714e-01, -8.6515e-01,  1.0522e-01,  1.1569e+00, -1.5143e-01,\n",
       "          -5.6470e-01, -2.4997e-02,  2.3636e-01,  2.2596e-01, -2.8055e-01,\n",
       "          -2.3398e-01, -9.7038e-01, -4.1627e-01, -6.1005e-01, -1.1292e+00,\n",
       "          -9.8677e-01, -8.2394e-02, -4.3003e-01, -4.2675e-01,  7.0611e-02,\n",
       "           3.7749e-01, -1.3126e-01,  2.5322e-02,  4.8668e-01,  4.4417e-01,\n",
       "           9.4967e-01,  7.6908e-01,  1.5242e-01,  3.1926e-01, -1.0061e-01,\n",
       "           3.9350e-02,  6.6734e-01,  2.6360e-01,  2.9753e-01,  7.9977e-01,\n",
       "           7.5140e-01,  2.9526e-01,  1.7352e-01,  5.9705e-01,  3.9471e-01]]),\n",
       " tensor(4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: Size should be [batch_size, num_features, feature_vector_len], [batch_size]\n",
    "# Where batch size is 32, num_features (num mfccs) is 40 \n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "features = train_features[0]\n",
    "label = train_labels[0]\n",
    "features,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 100]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    x, y = batch\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits = model(X)\n",
    "        predictions = torch.argmax(output_logits,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits = model(X)\n",
    "            predictions = torch.argmax(output_logits,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable params:  1406471\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = LSTMnet_RnnAtten(input_dim = 100, hidden_dim=128, output_dim=7, num_layers=10).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.Adam(model.parameters(),lr=0.0001, weight_decay=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0: batch 17"
     ]
    }
   ],
   "source": [
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "\n",
    "losses=[]\n",
    "val_losses = []\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    for idx, (features, labels) in enumerate(train_dataloader):\n",
    "        X, Y = features, labels\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*len(features)/len(train)\n",
    "        epoch_loss += loss*len(features)/len(train)\n",
    "        print(f\"\\r Epoch {epoch}: batch {idx}\",end='')\n",
    "    for batch in val_dataloader:\n",
    "        X_val, Y_val = batch\n",
    "        break\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40a9768d250eb22e51b6c5a3b41fdf4e51420751aaeb4dd105d54b23da568f3d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.1 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
